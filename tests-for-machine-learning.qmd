---
title: "Tests for machine learning"
author: "Tiffany A. Timbers"
format:
  pdf:
    toc: true
    number-sections: true
bibliography: references.bib
---

## Context

Checklists have been shown to decrease errors 
in safety critical systems [@gawande2010checklist], 
and the use of a reproducibility checklist 
at the Machine Learning NeurIPS 2019 conference led to an increase 
in the percentage of authors submitting the code for their work 
[from 50% to 75%; @pineau2021improving]. 
Thus, in an effort to make applied machine learning software more trustworthy
by increasing its robustness, 
we aim to create a general and robust checklist 
for creating software tests for applied machine learning code.
Such a checklist may be used by data scientists and machine learning engineers
to guide the manual writing of tests.
It may also act as a source for engineering large-language model (LLM) prompts 
that act as reliable starting points for engineering reproducible test data 
and software tests themselves for each item on the checklist. 

Such a checklist should include tests for data presence, quality 
and ingestion at the beginning of the analysis, 
the model fitting and evaluation, 
as well as tests for the artifacts (presence and quality) 
which are created by the analysis. 
To write a comprehensive checklist for creating software tests 
for applied machine learning code we are researching industrial best practices
(e.g., as documented in published guides, such as @microsoftengplaybook,
and blog posts of experienced machine learning software engineers,
for example @efftestML),
as well as the published academic literature 
[@BRAIEK2020110542; @openja2023studying; @silva2023case].
@openja2023studying in particular is of significant interest 
as they comprehensibly studied the testing strategies 
of 10 open source machine learning projects;
and identified 15 major categories of testing strategies 
used by machine learning engineers and 13 different types of tests.
Furthermore, they quantified the frequency of these to identify
which are most common across the projects studied.

The identification and categorization testing strategies and types of tests
used by machine learning engineers in the wild [@openja2023studying]
is very informative as to which strategies and test types should be used
for future machine learning projects. 
Additionally, it would be wise to link each item 
on the comprehensive machine learning testing checklist we are developing 
to each test strategy and test type identified in @openja2023studying.
One outstanding question from this work however is 
how representative are those 10 machine learning projects 
studied by @openja2023studying?

It would be very informative and worthwhile to examine additional
machine learning projects to assess 
whether their findings do extend to other projects.
In particular projects that are more complete, 
such as published machine learning analyses,
as well as projects where machine learning is being applied 
as the type of analysis to answer predictive questions in a given domain.
There are two potential sources of data for this,
the first being a subset of the papers identified in @wattanakriengkrai2022github.
In that study, they linked 377 GitHub repositories to academic papers,
as well as coded each study as to what they were focused on 
(e.g., deep learning, computer vision, NLP, machine learning, sensors, etc).
This data set is open and available
[here](https://docs.google.com/spreadsheets/d/e/2PACX-1vQ7gkxqJpzsa4T2ZuZg-VfAvD9U4r-XbpmwdlkK3k_RZ5Jc8bhgLK6I6xHj_KSTHTZaoJg-SqcE2yPj/pubhtml).
These studies appear to primarily be methodology papers.
A second source for studies is the 
[xDD/GeoDeepDive API](https://geodeepdive.org/index.html).
This API aims to allow for the extraction of "dark data" from scientific works, 
which were previously only available from manually reading of the literature.
This API could be used to identify papers 
where machine learning is being applied as the type of analysis 
to answer predictive questions in a given domain.
It is very possible that both types of work 
(methodological machine learning studies 
and studies where machine learning is applied 
to answer a predictive question in a given domain)
use and need different types of tests and testing strategies.
This is currently unknown.

## Annotated checklist of tests for machine learning

> Below is a tentative checklist of items that should go on 
> the comprehensive checklist for applied machine learning projects. 
> It is by no means "comprehensive" at this point 
> and currently only includes ideas from @microsoftengplaybook and @efftestML.

#### Saving and loading data

- [ ] Loading data file function works as expected [@microsoftengplaybook].
- [ ] Saving data/figures function works as expected

#### Data Validation

- [ ] Files contain data [@microsoftengplaybook].
- [ ] Data/images in the expected format [@microsoftengplaybook].
- [ ] Data does not contain null values or outliers [@microsoftengplaybook].

#### Cleaning and transforming data

- [ ] Cleaning and transforming functions works as expected [@microsoftengplaybook].

#### Modeling (pre-train tests)

- [ ] Does the model accept the correct inputs and produce the correctly shaped outputs [@microsoftengplaybook]?
- [ ] Do the weights of the model update when running fit [@microsoftengplaybook]?
- [ ] Does model output aligns with expectations (for example, in classification, are the labels what are expected based on input) [@efftestML]?
- [ ] Do the output ranges align with our expectations (eg. the output of a classification model should be a distribution with class probabilities that sum to 1) [@efftestML]?
- [ ] Does a single gradient step on a batch of data yield a decrease in your loss [@efftestML]?
- [ ] Is there leakage between your training, validation and test datasets [@efftestML]?

#### Other potential tests for modeling

We can also write tests that assess 
whether the machine learning model logic is correct,
these tests are referred to as post-train tests, or behavioural tests 
[@efftestML, @ribeiro2020beyond].
As well as tests to assess model performance,
these tests are referred to as evaluation tests [@howtotestMLcode].
These evaluation tests may be particularly useful for 
determining data distribution shifts when models are in production.

## Machine learning testing examples

There exist several examples/demos of how to test machine learning code. 
The data set and code used in these examples, 
may be an excellent starting place for the first test case to assess if 
the checklist can be used by LLM's to generate reproducible test data 
and high quality test cases.

- [`testing-ml`](https://github.com/eugeneyan/testing-ml) by Eugene Yan and [the accompanying article](https://eugeneyan.com/writing/testing-ml/#model-evaluation-to-ensure-satisfactory-performance)
uses the Titanic data set
- [`mercury-robust`](https://github.com/BBVA/mercury-robust) by BBVA uses the Titanic, Tips and default credit card data sets
- [`breast_cancer_predictor_py`](https://github.com/ttimbers/breast_cancer_predictor_py) by Timbers *et al.* uses the Madison Wisconsin Breast Cancer data set

## Cookiecutter project templates

Having a function specifications for the functions 
needed for an applied machine learning project 
would be useful for pairing with the prompts 
for LLM test data and test suite generation.
Below is a potential Cookiecutter project templates 
that might be of interest for modification and/or extension 
with the checklist items and prompts:

- [`cookiecutter-data-science`](https://github.com/drivendata/cookiecutter-data-science) by [\@drivendata](https://github.com/drivendata) (very popular and well structured project with scripts and pipeline, no function specifications however) 

## How to assess test quality of automated tests

There are several ways test quality can be assessed:

1. Human expert quality control (QC) - a human expert reviews the test cases and evaluates the test suite quality and case coverage. This assessment can be of great utility and high quality, but it is manual and thus can be quite slow. This can also be limited by the availability of the human expert.
2. Coverage (e.g., branch or line coverage) - the lines, or branches, of code executed by the test suite is calculated as a percentage of the code base. This assessment can be completely automated, and is therefore very efficient, it however does not assess the quality of the test cases.
3. Mutation testing - the code under test is intentionally changed/mutated to induce bugs, and the test suite is evaluated for its ability to detect the mutations. This can be a nice balance between human expert QC and coverage, as although human expertise is needed to design and create the mutated version of the code base, once created, assessing it can be automated. See @8424982 for an example where this was used to assess the presence of bugs in Weka machine learning code.

## References