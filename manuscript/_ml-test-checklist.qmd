- Present the checklist

### Applied machine learning checklist

- Discuss possible threats to a robust and trustworthy applied machine learning 
project, possible issues include:

    - General errors in code (e.g., bug in code that leads to data labels being shifted by one, or misnaming a file being written to disk)
    - Mismatch of machine learning model choices with respect to the data used for training and evaluation (e.g., linear regression for binomial data)
    - Data quality issues (e.g., missing data, duplicate data, data anomalies, imbalances in categorical data, etc)
    - Data leakage between training and test set, leading to overfitting (e.g., test data being used to create pre-processing object)
    - Issues with hyperparameter tuning (e.g., too small of a search space when choosing hyperparameters)
    - General model performance (e.g., for example, in classification, are the labels what are expected based on input)
    - Model stability issues (e.g., a different train-validation split leads to a large change in the model)
    - Model behaviour/learning issues (e.g., model learns shortcut to predictions that can learn to erroneous prediction in certain cases)
    - Bias/fairness issues (e.g., model makes different predictions for particular subgroups of observations)
    - Reproducibility issues (e.g., model training and prediction outputs are different on different computers or operating systems)
    - Data drift (e.g., model performance decreases over time in production and the new data appears to being coming from a different distribution than the test data)
    - Communication of results/predictions issues and/or user interface issues (this itself is very very broad... and may have to be split out)

- Not all issues presented above can be adequately addressed 
through writing software tests,
and so we will reduce the list for the purpose of our software test checklist
for machine learning to the following issues:

1. Errors in code (e.g., bug in code that leads to data labels being shifted by one, or misnaming a file being written to disk)
2. Data quality issues (e.g., missing data, duplicate data, data anomalies, imbalances in categorical data, etc)
3. Data leakage between training and test set, leading to overfitting (e.g., test data being used to create pre-processing object)
4. General model performance (e.g., for example, in classification, are the labels what are expected based on input)
5. Model stability issues (e.g., a different train-validation split leads to a large change in the model)
6. Model behaviour/learning issues (e.g., model learns shortcut to predictions that can learn to erroneous prediction in certain cases)
7. Bias/fairness issues (e.g., model makes different predictions for particular subgroups of observations)
8. Data drift (e.g., model performance decreases over time in production and the new data appears to being coming from a different distribution than the test data)

#### Applied machine learning checklist items

1. Errors in code
    - [ ] Loading data/model file function works as expected [@microsoftengplaybook]
    - [ ] Saving data/model/figures function works as expected
    - [ ] Data cleaning and transforming functions works as expected [@microsoftengplaybook].

2. Data quality issues
    - [ ] Code checks that files contain data and that it is in the expected format [@microsoftengplaybook].
    - [ ] Code checks that data does not contain null values, duplicates, wrong types or outliers [@microsoftengplaybook].

3. Data leakage between training and test set, leading to overfitting
    - [ ] Data is split into training and test set
    - [ ] Code checks that there are no duplicate records in the training and test set
    - [ ] Pre-processor is only created from the test set
    
4. General model performance
    - [ ] The model accept the correct inputs and produce the correctly shaped outputs [@microsoftengplaybook]
    - [ ] The weights of the model update when running fit [@microsoftengplaybook]
    - [ ] The model output aligns with expectations [@efftestML]
    - [ ] The output ranges align with our expectations (eg. the output of a classification model should be a distribution with class probabilities that sum to 1) [@efftestML]

5. Model stability issues
    - [ ] Code checks the model weights stability during training (e.g., different training/validation/cross-validation splits shouldn't significantly change the weights)

6. Model behaviour/learning issues
    - [ ] Code checks for invariance for predictions [@efftestML, @ribeiro2020beyond]
    - [ ] Code checks for directionality of predictions [@efftestML, @ribeiro2020beyond]

7. Bias/fairness issues
    - [ ] ???

8. Data drift
    - [ ] Code checks/compares the distribution of features from the training data and the prediction data to determine whether they are different

- Use breast cancer toy example/demo to walk through 3-5 items from the checklist
(maybe one from each kind of test from the testing triangle?)

- Report evaluation of $n$ (not sure what number n should be...) 
applied machine learning projects in the wild with respect to the checklist 
(creates a benchmark)